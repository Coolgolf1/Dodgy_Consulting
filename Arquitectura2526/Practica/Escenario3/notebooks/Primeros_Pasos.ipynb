{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4787167-bca8-443c-880d-8f65c716adb1",
   "metadata": {},
   "source": [
    "# IMAT\n",
    "\n",
    "**Asignatura:** Arquitectura de Big Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a32aedc4-01fa-4e7f-8379-66841039da71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daff128a-ab30-4a00-9f06-d80bef1bf0fe",
   "metadata": {},
   "source": [
    "### VARIABLES DE ENTORNO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "213fb9c1-4835-4124-8e7d-a7faad88ee25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python3.9\n",
      "3.9.23 (main, Jun  4 2025, 08:55:38) \n",
      "[GCC 11.4.0]\n",
      "PYSPARK_PYTHON: /usr/bin/python3.9\n",
      "PYSPARK_DRIVER_PYTHON: /usr/bin/python3.9\n"
     ]
    }
   ],
   "source": [
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "\n",
    "print(\"PYSPARK_PYTHON:\", os.environ.get(\"PYSPARK_PYTHON\"))\n",
    "print(\"PYSPARK_DRIVER_PYTHON:\", os.environ.get(\"PYSPARK_DRIVER_PYTHON\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b638d6-e949-4605-8e68-9d7b375edc5a",
   "metadata": {},
   "source": [
    "### CREAMOS LA SESION DE SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22e4f545-ace7-4c57-b1d1-8d9a4a1fb8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "appdate = datetime.now().strftime(\"%m-%d-%Y %H:%M:%S\")\n",
    "appName = '-'.join(['Procesamiento', appdate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5576f4fe-5bc1-4b73-b55f-9c2e67eef3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/26 09:32:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Crear sesión Spark conectando al master del cluster\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(appName) \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8640e1c4-8f49-41f9-b1af-f90bec266de4",
   "metadata": {},
   "source": [
    "### LECTURA FICHERO USANDO PYTHON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abb2be56-5d35-4e40-9ee4-f21e2cf10d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = pd.read_csv('precios.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ba08b8a-915b-47a0-a56b-21db899b5f5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>Department</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Avg_Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ramesh</td>\n",
       "      <td>20</td>\n",
       "      <td>Finance</td>\n",
       "      <td>50000</td>\n",
       "      <td>40000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Suresh</td>\n",
       "      <td>22</td>\n",
       "      <td>Finance</td>\n",
       "      <td>50000</td>\n",
       "      <td>40000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ram</td>\n",
       "      <td>28</td>\n",
       "      <td>Finance</td>\n",
       "      <td>20000</td>\n",
       "      <td>40000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pradeep</td>\n",
       "      <td>22</td>\n",
       "      <td>Sales</td>\n",
       "      <td>20000</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Deep</td>\n",
       "      <td>25</td>\n",
       "      <td>Sales</td>\n",
       "      <td>30000</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Name  Age Department  Salary  Avg_Salary\n",
       "0   Ramesh   20    Finance   50000       40000\n",
       "1   Suresh   22    Finance   50000       40000\n",
       "2      Ram   28    Finance   20000       40000\n",
       "3  Pradeep   22      Sales   20000       25000\n",
       "4     Deep   25      Sales   30000       25000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429b3e59-16c5-486f-af59-ee4bed57b342",
   "metadata": {},
   "source": [
    "### LECTURA FICHERO PYSPARK/ OPERACIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17800eca-c55d-490f-8ec6-bf1a8c2442aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+----------+------+\n",
      "|   Name|Age|Department|Salary|Avg_Salary|maximo|\n",
      "+-------+---+----------+------+----------+------+\n",
      "| Ramesh| 20|   Finance| 50000|     40000| 50000|\n",
      "| Suresh| 22|   Finance| 50000|     40000| 50000|\n",
      "|    Ram| 28|   Finance| 20000|     40000| 50000|\n",
      "|Pradeep| 22|     Sales| 20000|     25000| 30000|\n",
      "|   Deep| 25|     Sales| 30000|     25000| 30000|\n",
      "+-------+---+----------+------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window as w\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "wt = w.partitionBy('Department')\n",
    "df = spark.read.csv('precios.csv', inferSchema=True, header=True)\n",
    "df = df.withColumn('maximo', F.max('Salary').over(wt))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5b5d58-afa6-4f3b-afbf-1059d6689e55",
   "metadata": {},
   "source": [
    "### PRUEBAS DE RENDIMIENTO 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b21888f-7a5e-4bcd-b7d5-58de37766c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 100000 registros\n",
      "1: 100000 registros\n",
      "2: 100000 registros\n",
      "3: 100000 registros\n",
      "4: 100000 registros\n",
      "5: 100000 registros\n",
      "6: 100000 registros\n",
      "7: 100000 registros\n",
      "8: 100000 registros\n",
      "9: 100000 registros\n",
      "10: 100000 registros\n",
      "11: 100000 registros\n",
      "12: 100000 registros\n",
      "13: 100000 registros\n",
      "14: 100000 registros\n",
      "15: 100000 registros\n",
      "16: 100000 registros\n",
      "17: 100000 registros\n",
      "18: 100000 registros\n",
      "19: 100000 registros\n",
      "20: 100000 registros\n",
      "21: 100000 registros\n",
      "22: 100000 registros\n",
      "23: 100000 registros\n",
      "24: 100000 registros\n",
      "25: 100000 registros\n",
      "26: 100000 registros\n",
      "27: 100000 registros\n",
      "28: 100000 registros\n",
      "29: 100000 registros\n",
      "30: 100000 registros\n",
      "31: 100000 registros\n",
      "32: 100000 registros\n",
      "33: 100000 registros\n",
      "34: 100000 registros\n",
      "35: 100000 registros\n",
      "36: 100000 registros\n",
      "37: 100000 registros\n",
      "38: 100000 registros\n",
      "39: 100000 registros\n",
      "40: 100000 registros\n",
      "41: 100000 registros\n",
      "42: 100000 registros\n",
      "43: 100000 registros\n",
      "44: 100000 registros\n",
      "45: 100000 registros\n",
      "46: 100000 registros\n",
      "47: 100000 registros\n",
      "48: 100000 registros\n",
      "49: 100000 registros\n",
      "50: 100000 registros\n",
      "51: 100000 registros\n",
      "52: 100000 registros\n",
      "53: 100000 registros\n",
      "54: 100000 registros\n",
      "55: 100000 registros\n",
      "56: 100000 registros\n",
      "57: 100000 registros\n",
      "58: 100000 registros\n",
      "59: 100000 registros\n",
      "60: 100000 registros\n",
      "61: 100000 registros\n",
      "62: 100000 registros\n",
      "63: 100000 registros\n",
      "64: 100000 registros\n",
      "65: 100000 registros\n",
      "66: 100000 registros\n",
      "67: 100000 registros\n",
      "68: 100000 registros\n",
      "69: 100000 registros\n",
      "70: 100000 registros\n",
      "71: 100000 registros\n",
      "72: 100000 registros\n",
      "73: 100000 registros\n",
      "74: 100000 registros\n",
      "75: 100000 registros\n",
      "76: 100000 registros\n",
      "77: 100000 registros\n",
      "78: 100000 registros\n",
      "79: 100000 registros\n",
      "80: 100000 registros\n",
      "81: 100000 registros\n",
      "82: 100000 registros\n",
      "83: 100000 registros\n",
      "84: 100000 registros\n",
      "85: 100000 registros\n",
      "86: 100000 registros\n",
      "87: 100000 registros\n",
      "88: 100000 registros\n",
      "89: 100000 registros\n",
      "90: 100000 registros\n",
      "91: 100000 registros\n",
      "92: 100000 registros\n",
      "93: 100000 registros\n",
      "94: 100000 registros\n",
      "95: 100000 registros\n",
      "96: 100000 registros\n",
      "97: 100000 registros\n",
      "98: 100000 registros\n",
      "99: 100000 registros\n",
      "\n",
      "Tiempo total: 1.26 segundos\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Inicio del test\n",
    "inicio = time.time()\n",
    "\n",
    "# Crear un DataFrame de 10 millones de números\n",
    "df = spark.range(0, 10_000_000)\n",
    "\n",
    "# Simular carga: filtrar, transformar y agrupar\n",
    "resultado = df \\\n",
    "    .withColumn(\"mod\", (df.id % 100)) \\\n",
    "    .groupBy(\"mod\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"mod\")\n",
    "\n",
    "# Acción: traer los resultados al driver\n",
    "resultado_local = resultado.collect()\n",
    "\n",
    "fin = time.time()\n",
    "\n",
    "# Mostrar los resultados y tiempo\n",
    "for row in resultado_local:\n",
    "    print(f\"{row['mod']}: {row['count']} registros\")\n",
    "\n",
    "print(f\"\\nTiempo total: {fin - inicio:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b03004-f7f9-4874-8c7b-76c3555375b4",
   "metadata": {},
   "source": [
    "### PRUEBAS DE RENDIMIENTO 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fd1344a-0e16-4e9b-b24b-4d43c8a14991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks executed\n",
      "    10000 tasks on 172.19.0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import socket\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "# Función que simula el trabajo y devuelve la IP del nodo que ejecuta la tarea\n",
    "def get_ip(_):\n",
    "    time.sleep(0.001)\n",
    "    ip = socket.gethostbyname(socket.gethostname())\n",
    "    return ip\n",
    "\n",
    "# Crear un RDD con 10,000 elementos (simula 10,000 tareas)\n",
    "rdd = sc.parallelize(range(10000))\n",
    "\n",
    "# Mapear la función get_ip a cada elemento\n",
    "ip_addresses = rdd.map(get_ip).collect()\n",
    "\n",
    "# Contar cuántas tareas ejecutó cada IP (nodo)\n",
    "counter = Counter(ip_addresses)\n",
    "\n",
    "print(\"Tasks executed\")\n",
    "for ip, num_tasks in counter.items():\n",
    "    print(f\"    {num_tasks} tasks on {ip}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94af9bef-b46b-49da-91c6-27d4793898e9",
   "metadata": {},
   "source": [
    "### Finalizamos SPARK SESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c266f622-f8f7-4f93-bf95-6250f27e710f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (Custom)",
   "language": "python",
   "name": "python39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
